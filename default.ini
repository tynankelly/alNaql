# =================================================================
# File System Configuration
# =================================================================
[file]
# Base directory containing source files to be processed
# These will be analyzed for n-grams and similarity matching
source_dir = "./data/source"

# Directory containing target files to compare against
# Source files will be compared against these for similarity
target_dir = "./data/target"

ngrams_source_dir = "./ngrams/source"

ngrams_target_dir = "./ngrams/target"

# Directory where final results and reports will be written
# Contains match results and analysis outputs
output_dir = "./output"

# Directory for temporary files during processing
# Used for intermediate results and chunk processing
# Automatically cleaned up after processing completes
temp_dir = "./temp"

# =================================================================
# Text Processing Configuration
# =================================================================
[text_processing]
# Whether to remove numeric characters from text before processing
# Helps focus on textual content when numbers aren't significant
remove_numbers = true

# Whether to remove diacritical marks from Arabic text
# Normalizes text by removing harakat and other marks
# Useful for matching text with different diacritical patterns
remove_diacritics = true

# Whether to remove tatweel (kashida) from Arabic text
# Tatweel is used for text justification and doesn't affect meaning
remove_tatweel = true

# Whether to normalize various forms of Arabic letters
# e.g., different forms of alef (أ,إ,آ -> ا)
normalize_arabic = true

# Whether to keep punctuation marks in processed text
# False means punctuation will be removed during processing
preserve_punctuation = false

# List of words and phrases to be filtered out during ngram generation
# Needs to be a .txt file with one word/phrase per line
# Will be normalized along with xml files

stop_words_file = "filters/stop_words.txt"

# =================================================================
# N-gram Generator Configuration
# =================================================================
[generator]
# Type of n-grams to generate
# Options: "character" or "word"
# - "character" generates n-grams based on character sequences
# - "word" generates n-grams based on word sequences
ngram_type = word

# Size of n-grams to generate
# For character n-grams: number of characters in each n-gram
# For word n-grams: number of words in each n-gram
# Higher values create longer sequences but require more memory
ngram_size = 3

# Stride for n-gram generation - how many units (words/characters) to advance
# A stride of 1 gives maximum overlap (n-1), larger values reduce overlap
stride = 1

# Whether to use parallel processing for n-gram generation
# Significantly improves performance on multi-core systems
# This is not working yet
use_parallel = false

# Number of threads to use for parallel processing
# Default is number of CPU cores
# Set lower if memory usage is a concern
thread_count = 10

# Size in bytes of the buffer used for reading text files
# Larger values can improve I/O performance but use more memory
buffer_size = 524288

# Size in bytes of each text chunk processed
# Affects memory usage and processing granularity
chunk_size = 262144

# Level of log filter for ngram generation for debuggin
# Set to none, info, debug, warn, error, or trace
ngram_log = info

# =================================================================
# Processing Control Configuration
# =================================================================
[processor]
# Number of items to process in each batch
# Affects memory usage and processing efficiency
batch_size = 1000

# Processing mode: "sequential" or "parallel" processing of files
# Sequential is safer but slower, parallel offers better performance
processing_mode = "sequential"

# Maximum number of files to process concurrently
# Only applies in parallel mode
# 0 means use system-determined value based on available resources
max_concurrent_files = 2

# Memory threshold (%) at which to force sequential processing
# If system memory usage exceeds this, switches to sequential
force_sequential_memory_threshold = 95.0

# Target memory usage percentage
# System will try to maintain memory usage at this level
target_memory_percent = 90.0

# Enable/disable parallel ngram matching
parallel_ngram_matching = true

# Number of ngrams per parallel batch
parallel_batch_size = 10000

# Minimum file size for parallel
parallel_min_file_size = 65536

# Number of threads (0 = use num_cpus)
parallel_thread_count = 0

# =================================================================
# Pipeline Processing Configuration
# =================================================================

# Process ngrams through a piepline for large files
pipeline_processing = true

# Whether to automatically select between batch and pipeline processing
auto_select_strategy = true

# Minimum number of ngrams to consider pipeline processing
# Larger datasets benefit more from pipeline processing
pipeline_ngram_threshold = 5000

# Memory threshold ratio (estimated/available) for switching to pipeline
# If estimated memory usage is above this ratio of available memory,
# the system will use pipeline processing for better memory management
pipeline_memory_threshold = 0.7

# Minimum batch size for pipeline processing
# Smaller batches reduce memory usage but increase overhead
min_pipeline_batch_size = 500

# Maximum batch size for pipeline processing
# Larger batches are more efficient but use more memory
max_pipeline_batch_size = 2000

# Queue capacity for pipeline processing
# Controls how many batches can be queued between producer and workers
# Higher values improve throughput but use more memory
pipeline_queue_capacity = 16

# =================================================================
# Find_dense_regions parallel processing
# =================================================================

# Enable/disable parallel dense region finding
parallel_dense_regions = false

# Minimum number of matches to use parallel dense region finding
parallel_dense_threshold = 10000

# Minimum size of each chunk for dense region finding
min_dense_chunk_size = 500

# Maximum size of each chunk for dense region finding
max_dense_chunk_size = 2000

# =================================================================
# Banality/Isnad Detection Configuration
# =================================================================

# Banality detection mode: "phrase" or "automatic"
# - "phrase" uses predefined banality phrases from files
# - "automatic" uses statistical analysis of common n-grams
banality_detection_mode = "automatic"

# Percentage of most common n-grams to consider as potentially banal
# Higher values include more n-grams, lower values are more selective
# Range: 0.0 - 100.0
banality_auto_proportion = 10.0

# Threshold percentage for automatic banality detection
# Text is considered banal if this percentage of its n-grams are in the common set
# Range: 0.0 - 100.0
banality_auto_threshold = 80.0

# Whether to detect banal text early in the matching process
early_banality_detection = false

# Whether to log discarded matches for review
log_discarded_matches = true

# Path for logging discarded matches
discarded_log_path = logs/discarded_matches.log

# Whether to filter isnads early in detection process
early_isnad_detection = false


# =================================================================
# N-gram Matching Configuration
# =================================================================
[matcher]
# Maximum gap betwee matching ngrams in potential match
max_gap = 15

# Density of matching ngrams in a potential match
min_density = .3

# Word-based ngram matching parameters, only for non-exact ngram comparison (see below)
# Calculates how many words in an ngram must match before
word_match_min_ratio = .0

# Similarity metric for n-gram comparison
# Options: "exact", "jaccard", "cosine", "vsa", "levenshtein"
ngram_similarity_metric = "exact"

# Minimum confidence score for n-gram matches
# Value between 0 and 1
ngram_min_confidence = .9

# =================================================================
# Text Matching Configuration
# =================================================================
# Similarity metric for overall text comparison
# Options: "exact", "jaccard", "cosine", "vsa", "levenshtein"
text_similarity_metric = "cosine"

# In the [matcher] section of default.ini, add:
# Minimum confidence for initial text validation
initial_text_min_confidence = 0.6

# Minimum confidence for merged text validation
merged_text_min_confidence = 0.6

# Allowed gap between two matches that will be merged as a percentage 
# of their combined length
adjacent_merge_ratio = 0.5

# Phrase to trace through the matching process (leave empty to disable tracing)
traced_phrase = ""

# Minimum chain length for word n-grams 
min_word_length = 6

# Minimum number of characters required for a match
min_chars_length = 40

# =================================================================
# Proximity Matching Configuration
# =================================================================
# Whether to use proximity-based matching instead of strict sequential matching
# When true, allows ngrams to match in any order within proximity_distance. This
# is useful for matches with differences in syntax.
proximity_matching =true

# Maximum distance between ngrams in a proximity match
# Higher values allow more flexible matching but may increase false positives
proximity_distance = 10

# Density factor for proximity clusters (as fraction of min_density)
# Controls how densely packed matches must be in proximity mode
# Lower values require less density, higher values are stricter
proximity_density_factor = 0.3

# =================================================================
# Algorithm-Specific Configuration
# =================================================================
[matcher.algorithm]
# Minimum frequency for terms to be considered in cosine similarity
min_term_freq = 1

# Maximum vocabulary size for cosine similarity calculation
max_vocab_size = 10000

# Maximum edit distance for Levenshtein similarity
levenshtein_max_distance = 40

# Minimum term weight for Vector Space Analysis
vsa_min_term_weight = .01

# =================================================================
# Storage Configuration
# =================================================================
[storage]
# Base path for database files
db_path = "db/ngrams"

# =================================================================
# General Settings
# =================================================================
# Number of operations to batch together
batch_size = 1000

# Number of threads for parallel operations
parallel_threads = 8

# Whether to optimize for bulk loading
is_bulk_loading = false

# =================================================================
# I/O Configuration
# =================================================================
# Whether to use direct I/O (bypass OS cache)
# Can improve performance for large datasets
use_direct_io = true

# Whether to use fsync for writes
# True provides better durability but worse performance
use_fsync = false

# =================================================================
# Cache Configuration
# =================================================================
# Size of cache in megabytes
cache_size_mb = 8192

# =================================================================
# LMDB-Specific Configuration
# =================================================================
# Maximum map size in megabytes (pre-allocated database size)
# This is the most critical LMDB setting - must be large enough for all your data
# For large datasets, set this to at least 25% of RAM or larger
lmdb_map_size_mb = 10240  # 10GB default

# Maximum number of reader slots
# Increase for highly concurrent read access
lmdb_max_readers = 126

# Maximum number of named databases in the environment
# Includes main database, metadata database, and all prefix databases
lmdb_max_dbs = 100

# =================================================================
# Memory Mapping Configuration
# =================================================================
# Whether to allow memory mapping for reads
# Note: LMDB always uses memory mapping, this is for API compatibility
allow_mmap_reads = true

# Whether to allow memory mapping for writes
# Note: LMDB always uses memory mapping, this is for API compatibility
allow_mmap_writes = true